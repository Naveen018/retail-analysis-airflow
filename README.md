# Apache Spark ETL | End-to-End Data Engineering Project

## Table of Contents
- [Introduction](#introduction)
- [System Architecture](#system-architecture)
- [What You'll Learn](#what-youll-learn)
- [Technologies](#technologies)

## Introduction

This project serves as a comprehensive guide to building an end-to-end data engineering pipeline. It covers each stage from data ingestion to processing and finally to storage, utilizing a robust tech stack that includes Apache Airflow, Python, Apache Spark, AWS. Everything is containerized using Docker for ease of deployment and scalability.

## System Architecture


The project is designed with the following components:

- **Data Source**: Source of data is in mysql db.
- **Apache Airflow**: Responsible for orchestrating the pipeline.
- **Apache Spark**: For data processing with its master and worker nodes.

## What You'll Learn

- Setting up a data pipeline with Apache Airflow
- Data processing techniques with Apache Spark
- Data storage solutions with AWS s3 and PostgreSQL
- Containerizing your entire data engineering setup with Docker

## Technologies

- Apache Airflow
- Python
- Apache Spark
- MySQL
- PostgreSQL
- Docker
- Amazon s3
