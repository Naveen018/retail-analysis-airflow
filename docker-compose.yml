version: '3.8'

services:

  webserver:
    image: apache/airflow:2.6.0-python3.9
    command: webserver
    entrypoint: ['/opt/airflow/script/entrypoint.sh']
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW_WEBSERVER_SECRET_KEY=cd880d361c8061a721176fe72249d885
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jobs:/opt/airflow/jobs
      - ./jobs:/opt/spark/jobs
      - ./script/entrypoint.sh:/opt/airflow/script/entrypoint.sh
      - ./requirements.txt:/opt/airflow/requirements.txt
    ports:
      - "8070:8080"
    healthcheck:
      test: ['CMD-SHELL', "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      confluent:
        ipv4_address: 172.19.0.7

  scheduler:
    image: apache/airflow:2.6.0-python3.9
    depends_on:
      webserver:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jobs:/opt/airflow/jobs
      - ./jobs:/opt/spark/jobs
      - ./script/entrypoint.sh:/opt/airflow/script/entrypoint.sh
      - ./requirements.txt:/opt/airflow/requirements.txt
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW_WEBSERVER_SECRET_KEY=cd880d361c8061a721176fe72249d885
    command: bash -c "pip install -r ./requirements.txt && airflow db upgrade && airflow scheduler"
    networks:
      confluent:
        ipv4_address: 172.19.0.6

  postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    logging:
      options:
        max-size: 10m
        max-file: "3"
    networks:
      confluent:
        ipv4_address: 172.19.0.2

  spark-master:
    image: bitnami/spark:3.5.2
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./jobs:/opt/spark/jobs
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    #   SPARK_CONF_DIR: /opt/spark/conf
    #   SPARK_JARS: "/opt/spark/jars/*,/opt/spark/jars/hadoop-aws-3.2.0.jar,/opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar"
    networks:
      confluent:
        ipv4_address: 172.19.0.3

  spark-worker:
    image: bitnami/spark:3.5.2
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./jobs:/opt/spark/jobs
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
      - SPARK_MASTER_URL=spark://spark-master:7077
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      # SPARK_CONF_DIR: /opt/spark/conf
      # SPARK_JARS: "/opt/spark/jars/*,/opt/spark/jars/hadoop-aws-3.2.0.jar,/opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar"
    networks:
      confluent:
        ipv4_address: 172.19.0.4
  
  mysql:
    image: mysql:8.0.27
    container_name: mysql
    restart: always
    environment:
      MYSQL_DATABASE: 'retail_db'              # name of database
      MYSQL_USER: 'retailuser'                # sample is the name of user
      MYSQL_PASSWORD: 'root123'          # password for sample user
      MYSQL_ROOT_PASSWORD: 'root123'     # password for root user
    ports:
      - '4306:3306'                       # host port 3306 is mapper to docker port 3306
    expose:
      - '3306'
    volumes:
      - mysql-db:/var/lib/mysql
    networks:
      confluent:
        ipv4_address: 172.19.0.5

networks:
  confluent:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16
volumes:
  mysql-db:
